{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance tradeoff, CV, and variable selection #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find the closed form solution for $\\hat{\\beta} (\\lambda)$ and its distribution conditioning on ${x^i}$ (i.e., treat them as fixed). ####\n",
    "\n",
    "- Starting out with the gradient, $\\theta^r$ and setting it equal to 0 to find the closed form.\n",
    "- $\\frac{\\partial L(\\theta)}{\\partial\\theta} = -\\frac{2}{m}Xy+\\frac{2}{m}XX^T\\theta+2\\lambda\\theta =  0$\n",
    "- Dividing both sides by 2 and adding identity notation, we get\n",
    "- $=>$ $-\\frac{1}{m}Xy+\\frac{1}{m}XX^T\\theta+\\lambda I\\theta =  0$\n",
    "- Rearranging $Xy$ term, we get\n",
    "- $=>$ $\\frac{1}{m}XX^T\\theta+\\lambda I\\theta =\\frac{1}{m}Xy$\n",
    "- Factoring out $\\theta$, we get\n",
    "- $=>$ $(\\frac{1}{m}XX^T+\\lambda I)\\theta =\\frac{1}{m}Xy$\n",
    "- Applying the inverse of the first term on LHS, we get\n",
    "- $=>$ $\\theta^r = \\frac{1}{m}Xy(\\frac{1}{m}XX^T+\\lambda I)^{-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the bias $E[x^T\\hat{\\beta}(\\lambda)] - x^T\\beta*$ as a function of $\\lambda$ and some fixed test point $x$ ####\n",
    "- $=>$ $\\beta(xx^T)^{-1}x^Ty$\n",
    "- plugging in the definition of $\\beta$, we get\n",
    "- $=>$ $E[x^T(xx^T)^{-1}x(\\beta^Tx+\\epsilon)] - x^T\\beta$\n",
    "- after the cancellations, we get\n",
    "- $=>$ $E[\\beta^Tx]-x^T\\beta$ (the error term goes to 0 when taking expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the variance ####\n",
    "- Plugging in the definition of $\\beta$ and bias term from part b, we get\n",
    "- $E[x^T(xx^T)^{-1}x(\\beta^Tx+\\epsilon)] - E[\\beta^Tx]-x^T\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's suppose we have m = 100 samples. Write a pseudo-code to explain how to use cross validation to find the optimal $\\lambda$ ####\n",
    "- expected loss is (bias)^2+variance+noise, so partition the data into $K$ groups and compute total loss by computing sum of square of bias, variance, and noise\n",
    "- Find the optimal $H*$ that minimizes the expected loss, and this will be the best model with the optimal bias and variance tradeoff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}