{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "import numpy as np\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from math import log, floor, ceil\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Utility(object):\n",
    "    \n",
    "    # This method computes entropy for information gain\n",
    "    def entropy(self, class_y):\n",
    "        # Input:\n",
    "        # class_y: list of class labels (0's and 1's)\n",
    "\n",
    "        # ToDo: Compute the entropy for a list of classes\n",
    "        #\n",
    "        # Example:\n",
    "        #    entropy([0,0,0,1,1,1,1,1,1]) = 0.918 (rounded to three decimal places)\n",
    "\n",
    "        entropy = 0\n",
    "        ## calculate probability of each class (1/0)\n",
    "        pr_0 = len([y for y in class_y if y==0])/len(class_y)    \n",
    "        pr_1 = len([y for y in class_y if y==1])/len(class_y)\n",
    "        \n",
    "        ## if prob is 0 for either class, return untreated entropy\n",
    "        if pr_0 == 0 or pr_1 == 0:\n",
    "            return entropy\n",
    "\n",
    "        ## compute entropy\n",
    "        entropy = - pr_0 * np.log2(pr_0) - pr_1 * np.log2(pr_1)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def partition_classes(self, X, y, split_attribute, split_val):\n",
    "        # Inputs:\n",
    "        #   X               : data containing all attributes\n",
    "        #   y               : labels\n",
    "        #   split_attribute : column index of the attribute to split on\n",
    "        #   split_val       : a numerical value to divide the split_attribute\n",
    "\n",
    " \n",
    "\n",
    "        # ToDo: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "        \n",
    "        #   Splitting the data X into two lists(X_left and X_right) where the first list has all\n",
    "        #   the rows where the split attribute is less than or equal to the split value, and the \n",
    "        #   second list has all the rows where the split attribute is greater than the split \n",
    "        #   value. Also creating two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "\n",
    "        y_left = []\n",
    "        y_right = []\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if X[i][split_attribute] <= split_val:\n",
    "                X_left.append(X[i])\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(X[i])\n",
    "                y_right.append(y[i])\n",
    "\n",
    "        return (X_left, X_right, y_left, y_right)\n",
    "\n",
    "\n",
    "    def information_gain(self, previous_y, current_y):\n",
    "        # Inputs:\n",
    "        #   previous_y: the distribution of original labels (0's and 1's)\n",
    "        #   current_y:  the distribution of labels after splitting based on a particular\n",
    "        #               split attribute and split value\n",
    "\n",
    "        # ToDo: Compute and return the information gain from partitioning the previous_y labels\n",
    "        # into the current_y labels.\n",
    "        # Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "\n",
    "        info_gain = 0\n",
    "        # getting previous entropy\n",
    "        previous_entropy = self.entropy(previous_y)\n",
    "        ## getting a list of entropy values for splitting\n",
    "        after_entropy =[self.entropy(y) if len(y)>0 else 0 for y in current_y]\n",
    "        ## getting probabilities for split\n",
    "        probs = [len(y)/len(previous_y) for y in current_y]\n",
    "        ## getting the entropy after split\n",
    "        after_entropy = sum([after_entropy[i] * probs[i] for i in range(len(probs))])\n",
    "        ## getting the info gain\n",
    "        info_gain = previous_entropy - after_entropy\n",
    "\n",
    "        #############################################\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        # Inputs:\n",
    "        #   X       : Data containing all attributes\n",
    "        #   y       : labels\n",
    "        #   ToDo    : For each node find the best split criteria and return the split attribute, \n",
    "        #             spliting value along with  X_left, X_right, y_left, y_right \n",
    "        \n",
    "        split_attribute = 0\n",
    "        split_val = 0\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "        \n",
    "        num_attrs = len(X[0])\n",
    "        chosen_atrrs = np.random.choice(num_attrs,num_attrs//2, replace=False)\n",
    "        max = -float(\"Inf\")\n",
    "        return_dict = {}\n",
    "\n",
    "        ## find the split that maximizes info gain via iteration\n",
    "        for attr in chosen_atrrs:\n",
    "            for row in range(len(X)):\n",
    "                X_left_tmp, X_right_tmp, y_left_tmp, y_right_tmp = self.partition_classes(X, y, attr, X[row][attr])\n",
    "                current_y = [y_left_tmp, y_right_tmp]\n",
    "                info_gain = self.information_gain(y, current_y)\n",
    "\n",
    "                if max < info_gain:\n",
    "                    X_left, X_right, y_left, y_right = X_left_tmp, X_right_tmp, y_left_tmp, y_right_tmp\n",
    "                    split_attribute = attr\n",
    "                    split_value = X[row][attr]\n",
    "                    max = info_gain\n",
    "        \n",
    "        #insert outputs to a dictionary                    \n",
    "        return_dict['X_left'] = X_left\n",
    "        return_dict['X_right'] = X_right\n",
    "        return_dict['y_left'] = y_left\n",
    "        return_dict['y_right'] = y_right\n",
    "        return_dict['split_attribute'] = split_attribute\n",
    "        return_dict['split_value'] = split_value\n",
    "        return_dict['info_gain'] = info_gain\n",
    "        \n",
    "        return return_dict\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classes 'DecisionTree' and 'RandomForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        #self.tree = {}\n",
    "        self.tree = []\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    \t\n",
    "    def learn(self, X, y, depth=0):\n",
    "        # ToDo: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "       \n",
    "        util = Utility()\n",
    "        entropy_y = util.entropy(y)\n",
    "        if entropy_y == 0:\n",
    "            return [-1, y[0], None, None]\n",
    "\n",
    "        gain_best = 0\n",
    "        gain_best_idx = -1\n",
    "        value_min=[]\n",
    "        X_left_min=[]\n",
    "        X_right_min=[]\n",
    "        y_left_min=[]\n",
    "        y_right_min=[]\n",
    "        \n",
    "        length = len(X[0])\n",
    "\n",
    "        for curr_idx in range(0, length):      \n",
    "            local_val=np.mean([X[i][curr_idx] for i in range(len(X))])\n",
    "            X_left, X_right, y_left, y_right=util.partition_classes(X, y, curr_idx, local_val)\n",
    "            gain = util.information_gain(y, [y_left, y_right])\n",
    "\n",
    "            if gain > gain_best:\n",
    "                gain_best = gain\n",
    "                gain_best_idx = curr_idx\n",
    "                value_min = local_val\n",
    "                X_left_min, X_right_min, y_left_min, y_right_min= X_left, X_right, y_left, y_right\n",
    "        \n",
    "        if gain_best <= 0:\n",
    "            return [-1, np.argmax(np.bincount(y)),None, None]\n",
    "\n",
    "        left=self.learn(X_left_min,y_left_min)\n",
    "        right=self.learn(X_right_min, y_right_min)\n",
    "\n",
    "        self.tree=[gain_best_idx, value_min, left, right]\n",
    "\n",
    "        return self.tree\n",
    "\n",
    "    def classify(self, record):\n",
    "        # ToDo: classify the record using self.tree and return the predicted label\n",
    "        def classify_additional(record, tree):\n",
    "            idx, val, left, right = tree\n",
    "            if idx == -1:\n",
    "                return val\n",
    "            else:\n",
    "                if record[idx] <= val:\n",
    "                    return classify_additional(record, left)\n",
    "                else:\n",
    "                    return classify_additional(record, right)\n",
    "        return classify_additional(record, self.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# 1. X is assumed to be a matrix with n rows and d columns where n is the number of total records and d is the number of features of each record. \n",
    "# 2. y is assumed to be a vector of labels of length n.\n",
    "# 3. XX is similar to X, except that XX also contains the data label for each record.\n",
    "\n",
    "class RandomForest(object):\n",
    "    num_trees = 0\n",
    "    decision_trees = []\n",
    "\n",
    "    # bootstrapping datasets for trees\n",
    "    # bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "    bootstraps_datasets = []\n",
    "\n",
    "    # the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    # bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to records in\n",
    "    # the 'i'th bootstrapped dataset.\n",
    "    bootstraps_labels = []\n",
    "\n",
    "    def __init__(self, num_trees):\n",
    "        # Initialization done here\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [DecisionTree(max_depth=10) for i in range(num_trees)]\n",
    "        self.bootstraps_datasets = []\n",
    "        self.bootstraps_labels = []\n",
    "        \n",
    "    def _bootstrapping(self, XX, n):\n",
    "        # Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        #\n",
    "        # ToDo: Create a sample dataset of size n by sampling with replacement\n",
    "        #       from the original dataset XX.\n",
    "        # Note that you would also need to record the corresponding class labels\n",
    "        # for the sampled records for training purposes.\n",
    "\n",
    "        sample = [] # sampled dataset\n",
    "        labels = []  # class labels for the sampled records\n",
    "        \n",
    "        for i in range(n):\n",
    "            idx = np.random.randint(len(XX))\n",
    "            row = XX[idx]\n",
    "            sample.append(row[:-1]) #everything except last element\n",
    "            labels.append(row[-1]) #only last element\n",
    "\n",
    "        return (sample, labels)\n",
    "\n",
    "    def bootstrapping(self, XX):\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(XX, len(XX))\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "    def fitting(self):\n",
    "        # ToDo: Train `num_trees` decision trees using the bootstraps datasets\n",
    "        # and labels by calling the learn function from your DecisionTree class.\n",
    "        for i in range(self.num_trees):\n",
    "            self.decision_trees[i].learn(self.bootstraps_datasets[i], self.bootstraps_labels[i])\n",
    "        return(self.decision_trees[0].tree)\n",
    "\n",
    "    def voting(self, X):\n",
    "        y = []\n",
    "\n",
    "        for record in X:\n",
    "            #   1. Find the set of trees that consider the record as an\n",
    "            #      out-of-bag sample.\n",
    "            #   2. Predict the label using each of the above found trees.\n",
    "            #   3. Use majority vote to find the final label for this recod.\n",
    "            votes = []\n",
    "            \n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "                \n",
    "                if record not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify(record)\n",
    "                    votes.append(effective_vote)\n",
    "\n",
    "            counts = np.bincount(votes)\n",
    "\n",
    "            if len(counts) == 0:\n",
    "                #  Handle the case where the record is not an out-of-bag sample for any of the trees.\n",
    "                i = self.bootstraps_datasets[0].index(record)\n",
    "                y = np.append(y, self.bootstraps_labels[0][i])\n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "                               \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# ToDo: Determine the forest size according to your implementation. \n",
    "def get_forest_size():\n",
    "    forest_size = 10\n",
    "    return forest_size\n",
    "\n",
    "# ToDo: Determine random seed to set for reproducibility\n",
    "def get_random_seed():\n",
    "    random_seed = 0\n",
    "    return random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    np.random.seed(get_random_seed())\n",
    "    # start time \n",
    "    start = datetime.now()\n",
    "    X = list()\n",
    "    y = list()\n",
    "    XX = list()  # Contains data features and data labels\n",
    "    numerical_cols = set([i for i in range(0, 9)])  # indices of numeric attributes (columns)\n",
    "\n",
    "    # Loading data set\n",
    "    print(\"reading the data\")\n",
    "    with open(\"pima-indians-diabetes.csv\") as f:\n",
    "        next(f, None)\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            xline = []\n",
    "            for i in range(len(line)):\n",
    "                if i in numerical_cols:\n",
    "                    xline.append(ast.literal_eval(line[i]))\n",
    "                else:\n",
    "                    xline.append(line[i])\n",
    "\n",
    "            X.append(xline[:-1])\n",
    "            y.append(xline[-1])\n",
    "            XX.append(xline[:])\n",
    "\n",
    "    # Initializing a random forest.\n",
    "    randomForest = RandomForest(get_forest_size())\n",
    "\n",
    "\n",
    "    # Creating the bootstrapping datasets\n",
    "    print(\"creating the bootstrap datasets\")\n",
    "    randomForest.bootstrapping(XX)\n",
    "\n",
    "    # Building trees in the forest\n",
    "    print(\"fitting the forest\")\n",
    "    randomForest.fitting()\n",
    "\n",
    "    # Calculating an unbiased error estimation of the random forest\n",
    "    # based on out-of-bag (OOB) error estimate.\n",
    "    y_predicted = randomForest.voting(X)\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "    print(\"accuracy: %.4f\" % accuracy)\n",
    "    print(\"OOB estimate: %.4f\" % (1 - accuracy))\n",
    "\n",
    "    # end time\n",
    "    print(\"Execution time: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted RF-demo.ipynb to random-forest\\random-forest.py\n"
     ]
    }
   ],
   "source": [
    " %run helpers/notebook2script random-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading the data\n",
      "creating the bootstrap datasets\n",
      "fitting the forest\n",
      "accuracy: 0.7135\n",
      "OOB estimate: 0.2865\n",
      "Execution time: 0:00:00.731417\n"
     ]
    }
   ],
   "source": [
    "# Call the run() function to test your implementation\n",
    "# Use this cell and any cells below for additional testing\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python373jvsc74a57bd0176ed7074dac5392121fe20cc63b8a25e8cf34c6f1ec6a1ab2217b716782b9b2",
   "display_name": "Python 3.7.3 32-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "176ed7074dac5392121fe20cc63b8a25e8cf34c6f1ec6a1ab2217b716782b9b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}